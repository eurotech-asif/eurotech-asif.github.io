<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Asif Shaik — Resume</title>
<style>
  :root { color-scheme: light; }
  body { margin: 0; font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif; line-height: 1.5; color: #111; background: #fff; }
  .page { max-width: 900px; margin: 40px auto; padding: 0 20px 60px; }
  header { border-bottom: 1px solid #e5e5e5; padding-bottom: 16px; margin-bottom: 24px; }
  h1 { margin: 0 0 6px; font-size: 32px; letter-spacing: -0.02em; }
  .subtitle { margin: 0; color: #444; font-size: 16px; }
  h2 { margin: 26px 0 10px; font-size: 18px; text-transform: uppercase; letter-spacing: 0.06em; color: #222; }
  h3 { margin: 18px 0 8px; font-size: 16px; color: #222; }
  p { margin: 0 0 10px; }
  ul { margin: 8px 0 14px 20px; padding: 0; }
  li { margin: 6px 0; }
  .meta { display: flex; gap: 12px; flex-wrap: wrap; margin-top: 6px; color: #333; }
  .chip { background: #f3f4f6; border: 1px solid #e5e7eb; padding: 4px 10px; border-radius: 999px; font-size: 13px; }
  .two-col { display: grid; grid-template-columns: 1fr 1fr; gap: 18px; }
  @media (max-width: 720px) { .two-col { grid-template-columns: 1fr; } }
  .section { margin-top: 6px; }
  .small { font-size: 13px; color: #444; }
  a { color: inherit; }
</style>
</head>
<body>
<div class="page">

<header>
<h1>Asif Shaik</h1>
<p class='subtitle'>Azure Data Engineer</p>
<div class='meta'><span class='chip'><a href='mailto:asifsk1298@gmail.com'>asifsk1298@gmail.com</a></span><span class='chip'><a href='tel:+13167421414'>+1 316-742-1414</a></span></div>
</header>
<h2>Professional Summary</h2>
<p>Azure Data Engineer with over 10 years of experience building and improving data pipelines and ETL processes for healthcare, finance, retail, and banking companies.</p>
<p>Experienced in Agile teams, using JIRA to track work, Confluence for documentation, and ServiceNow for handling requests and workflows.</p>
<p>Skilled at designing data workflows that extract data from multiple sources, transform it into usable formats, and load it into storage or reporting systems — using Azure Data Factory, Apache Airflow, Informatica, Azure Data Factory, and Snowflake.</p>
<p>Comfortable working with Big Data tools like Hadoop, Hive, Sqoop, Flume, and Kafka to process and manage large datasets.y</p>
<p>Hands-on experience with Apache Spark and Spark Streaming to process data both in batches and in real time, helping teams get faster insights.</p>
<p>Well-versed in Azure services such as Redshift, Glue, and Azure Data Lake Storage Gen2 to build scalable and secure data solutions in the cloud.</p>
<p>Experienced using PySpark with Azure Data Factory to clean and transform large healthcare datasets, making them ready for analytics and fraud detection in Snowflake and Redshift.</p>
<p>Successfully migrated databases like Oracle and SQL Server from on-premise systems to Snowflake and Redshift, improving performance and accessibility.</p>
<p>Worked with Azure Blob Storage and Azure Data Factory to move, transform, and integrate high volumes of data.</p>
<p>Knowledgeable in data governance and compliance, ensuring systems follow HIPAA and FERPA regulations and keep sensitive data secure.</p>
<p>Strong skills in Python and SQL for data cleaning, automation, and writing optimized queries for reporting and analytics.</p>
<p>Created Linux shell scripts to automate data jobs, schedule workflows, and monitor systems.</p>
<p>Focused on testing, orchestration, and tuning performance to make sure data pipelines run reliably and on time.</p>
<p>Set up CI/CD pipelines with Jenkins, Terraform, and Azure DevOps to automate deployments and reduce manual errors.</p>
<p>Experienced using Git and GitHub to manage code versions, work on branches, and collaborate with teams.</p>
<h2>TECHNICAL SKILLS</h2>
<h2>Professional Track Record</h2>
<p>Client: Citi Bank										Nov 2024 – Till Date</p>
<p>Role: Senior Data Engineer									Location: New York</p>
<h2>Key Responsibilities</h2>
<p>Designed and optimized ETL and ELT pipelines using Azure Data Factory, Azure Functions, and Informatica to move data from Oracle, PostgreSQL, and streaming sources into Snowflake.</p>
<p>Built and maintained Snowflake data models, schemas, and data marts to support fraud detection, risk scoring, and compliance reporting.</p>
<p>Write and tuned advanced SQL queries and stored procedures in Snowflake, Oracle, and PostgreSQL to handle large financial datasets efficiently.</p>
<p>Managed Azure Data Lake Storage Gen2 data lakes, using partitioning, lifecycle rules, and access controls for secure and cost-efficient storage.</p>
<p>Implemented real-time streaming pipelines with Apache Kafka to process transactions and payment data for near-real-time fraud monitoring.</p>
<p>Developed PySpark pipelines for both batch and streaming data transformations, including data enrichment and feature preparation for analytics.</p>
<p>Deployed machine learning models using Azure Machine Learning for fraud detection, credit scoring, and customer segmentation, integrating results back into Snowflake for reporting.</p>
<p>Orchestrated data workflows with Azure Logic Apps and Lambda to enable automated, event-driven processing.</p>
<p>Queried data directly from Azure Data Lake Storage Gen2 using Azure Synapse Serverless SQL to enable quick, serverless analytics.</p>
<p>Automated data validation, reconciliation, and quality checks to meet SOX, Basel, and AML compliance requirements.</p>
<p>Migrated legacy Teradata and on-premise Oracle datasets to Snowflake, improving query performance and reducing reporting time.</p>
<p>Set up monitoring and alerting using Azure Monitor to track pipeline health and detect anomalies.</p>
<p>Containerized ETL jobs and services with Docker to ensure smooth deployment across multiple environments.</p>
<p>Managed code versioning and CI/CD pipelines with Git, GitHub, and Jenkins to support reliable and traceable releases.</p>
<p>Worked closely with risk, compliance, and analytics teams to deliver secure, scalable, and production-ready data solutions.</p>
<p>Client: Cigna Healthcare									      Aug 2022 - Oct 2024</p>
<p>Role: Senior Data Engineer							 	             Location: Bloomfield, CT</p>
<h2>Key Responsibilities</h2>
<p>Designed and maintained ETL pipelines using Azure Data Factory, Lambda, and Step Functions to process healthcare and claims data while following HIPAA rules.</p>
<p>Handled large volumes of patient data from multiple sources, using PySpark for distributed processing and Spark SQL for faster queries.</p>
<p>Built and supported data pipelines in Snowflake, Azure Synapse Analytics, and Spark SQL to make claims and healthcare data easy to analyze and report on.</p>
<p>Connected Snowflake with Azure Data Lake Storage Gen2 to simplify data loading, transformation, and storage between systems.</p>
<p>Tuned Snowflake performance to save costs and improve how quickly queries ran on large healthcare datasets.</p>
<p>Automated day-to-day workflows with Apache Airflow, tying together Lambda, Step Functions, and Docker jobs to run smoothly with minimal errors.</p>
<p>Added strong data checks in pipelines to keep information accurate and reliable across all systems.</p>
<p>Double-checked data transformation logic using PySpark and SQL to ensure results were consistent.</p>
<p>Wrote and improved SQL stored procedures and joins in PostgreSQL and SQL Server to make claims data processing faster.</p>
<p>Used Azure Monitor to watch over ETL jobs, quickly fixed issues, and kept systems running with little downtime.</p>
<p>Built real-time streaming pipelines with Apache Kafka and MongoDB so claims and patient data could be processed as it came in.</p>
<p>Set up infrastructure with Terraform to deploy Azure Virtual Machines, Azure Data Lake Storage Gen2, Lambda, and Docker resources in a scalable way.</p>
<p>Used Docker to package ETL jobs and microservices so they would run the same in all environments.</p>
<p>Created clear, visual Tableau dashboards that combined data from Redshift and Snowflake to show trends in claims and patient care.</p>
<p>Set up automated CI/CD pipelines using Jenkins and GitHub for smooth deployments and added SonarQube for checking code quality.</p>
<p>Improved older systems like Teradata and DynamoDB to make queries run faster and reports easier to generate.</p>
<p>Automated server tasks on Unix/Linux systems to keep pipelines stable and efficient.</p>
<p>Modeled and transformed complex EHR and healthcare data in Snowflake, making reporting and analytics faster and more accurate.</p>
<p>Improved EHR data processing by combining Azure Data Factory, Redshift, and PySpark, resulting in faster storage and querying.</p>
<p>Worked with FHIR and HL7 standards to turn healthcare data into consistent formats that different systems could use.</p>
<p>Collaborated with different teams using JIRA and Confluence to plan work, track progress, and document solutions in an Agile setup.</p>
<p>Client: State of KY 							 			Dec 2020 – July 2022</p>
<p>Role: Azure Data Engineer									Location: Remote</p>
<h2>Key Responsibilities</h2>
<p>Worked within an Agile environment, managing tasks and progress in Jira to ensure smooth delivery and meet state requirements on time.</p>
<p>Designed and developed ETL/ELT pipelines using Azure Data Factory and Informatica, transforming and loading data from Azure Data Lake Storage Gen2, Redshift, and Teradata to support financial reporting and analysis.</p>
<p>Built distributed data pipelines with Apache Spark, Spark SQL, and Spark Streaming to process both real-time transactions and batch data for state finance operations.</p>
<p>Used PySpark to process and transform large government financial datasets, improving the performance and efficiency of ETL processes.</p>
<p>Leveraged Databricks for collaborative development, testing, and advanced analytics, allowing teams to work together on large-scale data processing tasks.</p>
<p>Wrote and optimized SQL queries on PostgreSQL and Oracle databases for financial data extraction, reporting, and analytics to support budgeting and decision-making.</p>
<p>Automated data workflows using Azure Functions to trigger event-driven processing, which reduced manual steps and improved efficiency.</p>
<p>Used Azure Logic Apps to orchestrate complex ETL workflows, ensuring reliable and error-free data processing for critical financial transactions.</p>
<p>Queried data directly from Azure Data Lake Storage Gen2 using Azure Synapse Serverless SQL, allowing quick, serverless analysis without unnecessary data movement.</p>
<p>Implemented strict security policies with Azure Active Directory and Azure Key Vault to ensure sensitive financial and citizen data remained encrypted and accessible only to authorized users.</p>
<p>Managed Azure Service Bus and SNS to enable real-time messaging and alerts, improving the speed and reliability of financial transaction processing.</p>
<p>Migrated historical data from Teradata to Snowflake, increasing scalability and improving report performance for state agencies.</p>
<p>Set up Snowpipe for continuous ingestion of new data into Snowflake, enabling near real-time analytics and reporting for financial teams.</p>
<p>Containerized ETL jobs and microservices with Docker to ensure consistent deployments across multiple environments.</p>
<p>Used Git and GitHub for version control and collaboration, helping teams keep track of code changes and maintain release integrity.</p>
<p>Developed and maintained CI/CD pipelines with Jenkins and integrated SonarQube for code quality checks, automating build, test, and deployment cycles.</p>
<p>Performed unit testing and data validation on ETL pipelines to confirm accuracy, data integrity, and compliance with state financial reporting requirements.</p>
<p>Scheduled and automated ETL tasks using Apache Airflow, reducing manual intervention and improving process reliability.</p>
<p>Client: Best Buy											June 2019 – Nov 2020</p>
<p>Role: Data Engineer									    Location: Overland Park, KS</p>
<h2>Key Responsibilities</h2>
<p>Worked in an Agile setup, using Jira to track tasks and progress, ensuring smooth delivery and meeting deadlines.</p>
<p>Designed and developed ETL/ELT pipelines using AWS Glue and Informatica to pull data from S3, Redshift, and Teradata, transform it, and make it ready for reporting.</p>
<p>Built data pipelines using Apache Spark, Spark SQL, and Spark Streaming to process both real-time transactions and batch data for financial operations.</p>
<p>Used PySpark to handle large state financial datasets, speeding up processing and improving performance of ETL jobs.</p>
<p>Collaborated with teams using Databricks for developing, testing, and running analytics on large amounts of data.</p>
<p>Wrote and fine-tuned SQL queries on PostgreSQL and Oracle databases to get data needed for budgeting, reporting, and analysis.</p>
<p>Automated routine processes with AWS Lambda, reducing manual work and making data processing more efficient.</p>
<p>Used AWS Step Functions to connect different ETL jobs and ensure they ran in the right order with proper error handling.</p>
<p>Queried data directly from S3 using AWS Athena, allowing quick, serverless analysis without extra data movement.</p>
<p>Applied strict security policies with AWS IAM and AWS KMS to protect sensitive financial and citizen data.</p>
<p>Used AWS SQS and SNS to send alerts and messages in real time, helping with faster processing of financial transactions.</p>
<p>Migrated old data from Teradata to Snowflake, which improved scalability and reporting speed for financial teams.</p>
<p>Set up Snowpipe to continuously load data into Snowflake, giving near real-time insights to state agencies.</p>
<p>Containerized ETL jobs and services with Docker, ensuring consistent deployments in different environments.</p>
<p>Used Git and GitHub for version control, allowing smooth collaboration and tracking of code changes.</p>
<p>Built CI/CD pipelines with Jenkins and integrated SonarQube to check code quality and automate build, test, and deployment steps.</p>
<p>Performed unit testing and data validation to make sure all reports and pipelines were accurate and met compliance needs.</p>
<p>Automated job scheduling and monitoring using Apache Airflow, reducing manual efforts and improving reliability.</p>
<p>Client: Cyient											     Aug 2014 – Jan 2019</p>
<p>Role: Data Analyst                  								             Location: Hyderabad, IN</p>
<h2>Key Responsibilities</h2>
<p>Worked in an Agile setup, using JIRA to track tasks, report issues, and participate in sprint planning to keep project delivery on schedule.</p>
<p>Designed and developed ETL pipelines using Amazon S3, Amazon EC2, and Hadoop to process and store high volumes of financial transaction data efficiently.</p>
<p>Performed database administration tasks in SQL Server and MySQL, including indexing, partitioning, and backups to improve system performance and reliability.</p>
<p>Implemented Apache Sqoop to move structured data between SQL Server and HDFS, making financial data more accessible for downstream analytics.</p>
<p>Processed large data batches using MapReduce, improving the speed and accuracy of transaction analysis.</p>
<p>Managed distributed storage and retrieval of financial records using HDFS, ensuring data scalability and fault tolerance.</p>
<p>Created real-time data ingestion pipelines using Apache Flume to capture and process log data from financial systems.</p>
<p>Scheduled and optimized data workflows using YARN, ensuring efficient use of cluster resources and faster processing times.</p>
<p>Wrote and optimized SQL queries in SQL Server and MySQL to extract, transform, and analyze financial data for reporting and regulatory compliance.</p>
<p>Used Amazon RDS to host and manage relational databases, focusing on high availability and secure access to financial data.</p>
<p>Monitored data pipelines and system performance with Amazon CloudWatch, identifying bottlenecks and resolving them quickly.</p>
<p>Used Amazon SQS to enable asynchronous messaging between services, making data pipelines more scalable and reliable.</p>
<p>Automated repetitive ETL tasks using Python scripts for data validation, cleaning, and transformation, reducing manual work and improving accuracy.</p>
<p>Built dashboards and reports in Tableau and QlikView to give stakeholders clear insights into financial operations, trends, and risk exposure.</p>
</div>
</body>
</html>